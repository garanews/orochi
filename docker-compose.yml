version: "3"

volumes:
  local_postgres_data: {}
  local_postgres_data_backups: {}
  media_path: {}
  symbols_path: {}
  data01:
    driver: local

services:
  django: &django
    build:
      context: .
      dockerfile: ./compose/local/django/Dockerfile
      args:
        - http_proxy
        - https_proxy
    image: garanews/orochi_local_django:latest
    container_name: orochi_django
    depends_on:
      - postgres
      - mailhog
    volumes:
      - .:/app
      - media_path:/media
      - symbols_path:/src/volatility/volatility/symbols
    env_file:
      - ./.envs/.local/.django
      - ./.envs/.local/.postgres
    ports:
      - "8000:8000"
    command: /start

  postgres:
    build:
      context: .
      dockerfile: ./compose/local/postgres/Dockerfile
    image: garanews/orochi_local_postgres:latest
    container_name: orochi_postgres
    volumes:
      - local_postgres_data:/var/lib/postgresql/data
      - local_postgres_data_backups:/backups
    env_file:
      - ./.envs/.local/.postgres
    ports:
      - "5432:5432"

  mailhog:
    image: mailhog/mailhog:v1.0.1
    container_name: orochi_mailhog
    ports:
      - "8025:8025"

  redis:
    image: redis:6.0.6
    container_name: orochi_redis
    ports:
      - "6379:6379"

  es01:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.9.0
    container_name: orochi_es01
    environment:
      - node.name=es01
      - cluster.name=es-docker-cluster
      - cluster.initial_master_nodes=es01
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - data01:/usr/share/elasticsearch/data
    ports:
      - 9200:9200

  kib01:
    image: docker.elastic.co/kibana/kibana:7.9.0
    container_name: orochi_kib01
    ports:
      - 5601:5601
    environment:
      ELASTICSEARCH_URL: http://es01:9200
      ELASTICSEARCH_HOSTS: http://es01:9200

  scheduler:
    build:
      context: .
      dockerfile: ./compose/local/dask/Dockerfile
      args:
        - http_proxy
        - https_proxy
    image: garanews/orochi_scheduler:latest
    container_name: orochi_scheduler
    hostname: dask-scheduler
    env_file:
      - ./.envs/.local/.dask
    ports:
      - "8786:8786"
      - "8787:8787"
    command: ["dask-scheduler"]

  worker01:
    build:
      context: .
      dockerfile: ./compose/local/dask/Dockerfile
      args:
        - http_proxy
        - https_proxy
    image: garanews/orochi_worker01:latest
    container_name: orochi_worker01
    command:
      [
        "dask-worker",
        "--preload",
        "/tmp/daskworkerinit.py",
        "--nthreads",
        "1",
        "--nprocs",
        "2",
        "--memory-limit",
        "3G",
        "tcp://scheduler:8786",
      ]
    volumes:
      - .:/app
      - media_path:/media
      - symbols_path:/src/volatility/volatility/symbols
    env_file:
      - ./.envs/.local/.django
      - ./.envs/.local/.postgres
      - ./.envs/.local/.dask

  worker02:
    build:
      context: .
      dockerfile: ./compose/local/dask/Dockerfile
      args:
        - http_proxy
        - https_proxy
    image: garanews/orochi_worker02:latest
    container_name: orochi_worker02
    command:
      [
        "dask-worker",
        "--preload",
        "/tmp/daskworkerinit.py",
        "--nthreads",
        "1",
        "--nprocs",
        "2",
        "--memory-limit",
        "3G",
        "tcp://scheduler:8786",
      ]
    volumes:
      - .:/app
      - media_path:/media
      - symbols_path:/src/volatility/volatility/symbols
    env_file:
      - ./.envs/.local/.django
      - ./.envs/.local/.postgres
      - ./.envs/.local/.dask
